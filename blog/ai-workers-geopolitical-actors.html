<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />

  <title>AI Workers Are Becoming Geopolitical Actors — Sydney Reis</title>
  <meta name="description" content="Why AI workers matter for AI governance: a participatory, bottom-up approach to complement top-down regulation.">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&family=VT323&display=swap" rel="stylesheet">

  <!-- Your global styles are already on your site; these are minimal post styles -->
  <style>
    body {
      font-family: 'VT323', monospace;
      background: #0000a0;
      color: #fff;
      margin: 0;
      padding: 32px;
      max-width: 900px;
      line-height: 1.6;
      font-size: 18px;
      cursor: url('https://cur.cursors-4u.net/cursors/cur-2/cur114.cur'), default;
    }
    h1, h2 {
      font-family: 'Press Start 2P', monospace;
      color: #fff;
      text-transform: uppercase;
      letter-spacing: 1px;
      font-weight: normal;
    }
    h1 { font-size: 1.4rem; margin-top: 0; }
    h2 { font-size: 1.1rem; margin-top: 2em; }
    a {
      color: #0000a0;
      background: #fff;
      padding: 1px 4px;
      text-decoration: none;
      font-weight: bold;
      box-shadow: 1px 1px 0 #666 inset;
    }
    a:hover {
      background: #ffc;
      color: #0000a0;
    }
    article {
      background: rgba(255,255,255,0.05);
      padding: 20px;
      border: 1px solid rgba(255,255,255,0.2);
      box-shadow: 4px 4px 0 rgba(0,0,0,0.6);
    }
    .meta { color: #ccc; margin-bottom: 1.2rem; }
    nav.top-nav a.tab { cursor: url('https://cur.cursors-4u.net/cursors/cur-3/cur248.cur'), pointer; }
    footer {
      text-align: center;
      margin-top: 3rem;
      padding-top: 1rem;
      border-top: 1px solid #fff;
    }
  </style>
</head>

<!-- This highlights the BLOG tab automatically -->
<body data-active="blog">
  <main>

    <!-- NAVIGATION copied from your main site -->
    <header>
      <nav class="top-nav">
        <a class="tab" href="../index.html" data-nav="home">Home</a>
        <a class="tab" href="index.html" data-nav="blog">Blog</a>
      </nav>
</header>
    </header>

    <!-- BLOG POST CONTENT -->
    <article>
      <h1>AI Workers Are Becoming Geopolitical Actors, and it Matters for the Future of AI Governance</h1>

      <p class="meta">By Sydney Reis · Responsible Technology Institute, University of Oxford · 2025</p>

      <p>This blog post is based on a paper submitted to the 2025 NeurIPS workshop on Algorithmic Collective Action. The full paper and all of its citations are <a href="#" target="_blank" rel="noopener">here</a>.</p>

      <h2>Why AI Governance Alone Isn’t Enough</h2>

      <p>We talk a lot about regulating Artificial Intelligence (AI), but far less about the political power of the people who actually build, promote, and proliferate it. As AI systems have grown more powerful, so have the companies that develop them. These companies now influence national security, foreign policy, and even concepts of sovereignty. AI companies are also increasingly finding themselves at the center of the so-called global “AI arms race”. The theory of International Political Economy (IPE) shows that governments often find themselves relying on these firms rather than regulating them.</p>

      <p>According to IPE, states are often incentivized to remove barriers for powerful corporations if those corporations help advance national strategic interests. When it comes to AI, this means governments are torn between mitigating unintended AI harms and winning geopolitical advantage.</p>

      <p>If you’ve ever wondered why AI governance is uneven, slow, or in some cases, deprioritized, the geopolitical environment is partly to blame. But this doesn’t mean we’re stuck. Nor does it mean that AI governance isn’t the answer: in many cases, it absolutely is. It just means that we need to look beyond legislation and consider a different set of actors. This is where AI workers themselves come in.</p>

      <h2>AI Workers as Geopolitical Actors</h2>

      <p>We tend to imagine geopolitics as something governments do, not something software engineers or machine learning researchers influence. But I, and many others, argue that this view requires updating.</p>

      <p>AI workers sit in uniquely consequential positions. They design, train, and deploy the models shaping international AI competition, hold scarce and highly mobile technical skills, encode assumptions and values into the systems they build, and possess political agency (with a track record of using it).</p>

      <p>From Google employees walking out over Project Maven to workers protesting military cloud contracts, AI labour has already influenced geopolitics. Whether they see themselves this way or not, AI workers are becoming geopolitical actors simply by virtue of their power and influence over the systems they contribute to creating, while their technical artefacts contribute to the growth of companies that now rival states in terms of their resources. As AI companies increasingly shape national technology, military, and diplomatic strategy, workers’ decisions regarding what to build, what to refuse, and what to question matters more than ever.</p>

      <h2>A Gap in AI Governance</h2>

      <p>Most proposals designed to mitigate AI’s risks focus on the development of top-down interventions such as international treaties, national regulations, corporate governance guidelines, and safety standards. These types of governance are all important and necessary. But they are not enough, and there are other, arguably overlooked ways to complement AI governance that merit more attention.</p>

      <p>As long as states are incentivized to pursue geopolitical advantage, then governance mechanisms alone are not enough to ensure that the big words that we like to see around AI are actually operationalised; words like ethical, responsible, fair, safe, and secure. That is where local, bottom-up approaches come in, and where Algorithmic Collective Action (ACA) becomes an interesting use-case.</p>

      <h2>How AI Workers Can Contribute to Algorithmic Collective Action</h2>

      <p>ACA research typically looks at platform workers like artists, delivery drivers, and content moderators, some of whom may be marginalized in several ways, like being from the Global South or facing precarious labour conditions. Using algorithms, these groups organize to create better or fairer working conditions for themselves: hence the term Algorithmic Collective Action.</p>

      <p>However, AI workers that are arguably more privileged, such as researchers, engineers, and technical staff within corporate or university labs, rarely appear in ACA discussions. Science and Technology Studies (STS) has long recognized how the values of technology makers are reflected in technologies themselves. From this lens, it becomes impossible to ignore the role of AI researchers and engineers because of how they as people influence model development. Their collective actions can shape entire technological trajectories, and their workplaces have real geopolitical stakes and consequences. And unlike platform workers, AI workers sometimes enjoy mobility, bargaining power, and public visibility, all of which can make collective action that much easier to organize.</p>

      <h2>A Participatory Design Approach to Engaging AI Workers</h2>

      <p>If AI workers are geopolitical actors, how do we support them in acting responsibly or encouraging forms of resistance, such as through ACA? One path forward comes from Participatory Design (PD), a methodology rooted in empowering workers to shape technologies and practices that affect their lives. PD traditionally focuses on giving underrepresented groups a voice in technology design processes, but the practice could be applied to any group with specific needs and interests.</p>

      <p>For instance, PD could be used to help AI workers reflect on the geopolitical implications of their work, co-create tools and practices that support ethical decision-making, or build shared understanding and critical awareness of the structural or geopolitical influences of their technologies (the effects of which can be notoriously difficult to imagine!). PD itself is a bottom-up approach and a useful complement or alternative to governance. It doesn’t dictate or purport to have all the solutions. Rather, the approach is guided by an ethos of uncovering solutions with the people directly involved.</p>

      <p>AI workers are their own people, as any person is. They have their own preconceived values, notions, beliefs, and cultures… and that’s just at the tip of the iceberg. This isn’t about telling anyone what to believe. It is instead about finding ways for AI workers to consider things like: how do my design choices intersect with the global environment? How does my company’s position shape geopolitical outcomes? What forms of resistance are available to me? These are difficult questions for anyone. But with the retreat of formal governance, more and more “everyday people” ought to be asking them.</p>

      <h2>Better AI Needs a Bottom-Up Layer</h2>

      <p>The point isn’t to romanticize tech workers, nor is it to make them shoulder the burden of weak, politicised, or insufficient regulation. But they do have relative agency, insider knowledge, and are the architects of increasingly globally consequential systems. Any realistic strategy for safer, more just, more responsible, or just better AI needs to include them. Bottom-up interventions, from PD-informed practices to ACA, can and should complement top-down governance.</p>

      <p><em>— Sydney Reis</em></p>
    </article>

    <footer>
      <p>
        <a href="mailto:sydney.reis@wolfson.ox.ac.uk">email</a> ·
        <a href="https://www.linkedin.com/in/sydneyreis/" target="_blank">linkedin</a> ·
        <a href="https://www.are.na/sydney-reis/channels" target="_blank">are.na</a>
      </p>
    </footer>

  </main>
</body>
</html>
